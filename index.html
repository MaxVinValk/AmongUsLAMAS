<!DOCTYPE html>
<html>
<head>
    <script type="text/javascript"
    src="https://www.maths.nottingham.ac.uk/plp/pmadw/LaTeXMathML.js">
    </script>

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>

    body {
    margin: 0;
    font-family: Arial, Helvetica, sans-serif;
    }

    figure {
    border: 1px #cccccc solid;
    padding: 4px;
    margin: auto;
    text-align: center;
    }

    .row {
    display: flex;
    }

    .column {
    flex: 50%;
    padding: 5px;
    }

    figcaption {
    background-color: black;
    color: white;
    font-style: italic;
    padding: 2px;
    text-align: center;
    }

    .top-container {
    padding: 100px;
    background-image:url('site_images/AmongUs.jpeg');
    background-repeat: no-repeat;
    background-position: center;
    background-size: 100%;
    position: relative;
    object-fit: cover;
    width: auto;
    height: 150px;
    }

    .transbox {
    padding: 1px;
    opacity: 0.8;
    margin: 30px;
    text-align: center;
    color: #ffffff;
    background-color: black;
    }

    .topnav {
    overflow: hidden;
    background-color: #333;
    }

    .topnav a {
    float: left;
    color: #f2f2f2;
    text-align: center;
    padding: 14px 16px;
    text-decoration: none;
    font-size: 17px;
    }

    .topnav a:hover {
    background-color: #ddd;
    color: black;
    }

    .topnav a.active {
    background-color: #04AA6D;
    color: white;
    }

    .content {
    padding: 16px;
    }

    .sticky {
    position: fixed;
    top: 0;
    width: 100%;
    }

    .sticky + .content {
    padding-top: 102px;
    }
    </style>
    </head>
<body>

<div class="top-container">
<div class="transbox" >
<h3>Among Us</h3>
<p>Group 19: Logical Aspects of Multi-Agent Systems.</p>
</div>
</div>

<div class="topnav" id="myHeader">
        <a href="#what_is_among_us">Introduction</a>
        <a href="#formal_model">Model</a>
        <a href="#knowledge_types">Knowledge</a>
        <a href="#reasoning_rules">Rules</a>
        <a href="#findings">Findings</a>
        <a href="#extended-knowledge-model-movement-between-rooms">Extensions</a>
        <a href="https://github.com/MaxVinValk/AmongUsLAMAS">GitHub</a>
</div>

<div class="content">
  <a id="what_is_among_us"></a>
  <h2>What is Among Us?</h2>
    <p>Among us is a hidden information game which is set in various sci-fi locations. The players are divided up into two different groups, crewmates and impostors, which are visually indistinguishable. The goal of the impostors is to kill every crewmate, and the goal of the crewmates is to either complete a series of tasks in their environment, or to figure out who the impostors are. The latter is done by a discussion and voting round in which all players that are alive can share information. If during the voting round sufficient players vote for one specific player, they will be removed from the game.</p>
    <p>Information can be obtained in various ways. It is known to impostors who the other impostors are. Crewmates can obtain this information by witnessing certain acts that only impostors can perform, such as killing or making use of a special navigation system only accessible to them. Furthermore, crewmates can witness fellow cremates performing certain actions which confirm their innocence. These actions are special tasks that have a special animation which is visible to all players near the acting crewmate. Lastly, impostors have access to a special series of actions called sabotages. These actions do not have animations or any other indicators towards who performed them, so they can be performed without revealing who the impostors are, and help them in isolating crewmates.</p>
    <p>The results of this project consist of two parts. First, a simulation was built of a simplified version of the game. In this version, impostors do not have access to a special navigation system or sabotage, and do not interfere with the discussion phase. For the corresponding Kripke model, the information that is encoded within is which agent suspects which other agents of potentially being an impostor. Second, a formal model was set up, including a proof-of-concept program, exploring the possibilities of modelling information regarding to location and the use of special navigation systems.</p>
    <p>This report is structured as follows: First, we will discuss the formalization of the kripke model for the simulation. Second, we discuss how agents can interact with this kripke model. Third, we will discuss the findings from running the simulation with varying parameter settings. Finally, we will discuss the location model and its results.</p>

    <h3>Formalizing Among Us</h3>
    <p>Fig.1 showcases in broad terms how our implementation of Among Us is structured. Below, the phases are explained in more detail.</p>
    <figure>
      <img src="site_images/logic/program_flow.png" alt="A flowchart of how our Among Us is implemented." style="max-width: 100%; height: auto; width: 800px">
      <figcaption>Fig.1 - A flowchart of how Among Us is implemented.</figcaption>
    </figure>

    <h4>Action Phase</h4>
    <p>The action phase consists out of two sub-phases: acting and observing. In the former, all agents can either move to an adjacent room or perform an action, which is a task if they are a crewmate, or kill if they are an impostor. They can also choose to neither move nor perform an action. After all actions are completed, the agents observe what actions have taken place in the room that they are in, and witness those that are marked as observable events, such as visual tasks, murder or the presence of a corpse. The agents follow a hard-coded strategy to complete their own tasks by selecting one that they have not completed, moving to the correct room, and performing said task. Impostors, on the other hand, move about in a random fashion if they do not kill. This action phase is repeated until a crewmate observes a dead body, in which case the agent can report the body. This immediately triggers the discussion phase.</p>
    <p>To model the real game, the impostor will have a cooldown of a set number of rounds on their kill-action. When it is capable of killing a crewmate, this action is stochastic. This means that simply being in the same room as another agent without getting killed does not mean that they are a crewmate. The probability of the impostor killing a crewmate depends on how many other crewmates are in the room. If, for example, there are many other crewmates in the same room as the impostor, the chances of the impostor killing anyone are lower than when the impostor is in a room with only one crewmate. </p>

    <h4>Discussion Phase</h4>
    <p>The discussion phase starts with an announcement of which agent has died. Afterwards, each crewmate announces their entire knowledge set about who which agents are an impostor, a crewmate, or could still be both. At the start of the game, when crewmates still have not gained any knowledge, the knowledge of crewmate 1 is modelled like <span class="math inline">K_1 ((Imp_2 \lor \neg Imp_2) \land (Imp_3 \lor \neg Imp_3) \land ... \land (Imp_n \lor \neg Imp_n))</span>. Here, both possibilities are still open for each agent and the role of the crewmate making the announcement is not included.</p>
    <p>Then, when an agent knows that for example <span class="math inline">a_2</span> is an impostor and <span class="math inline">a_3</span> is a crewmate, the knowledge gets updated to <span class="math inline">K_1 ((Imp_2) \land (\neg Imp_3) \land ... \land (Imp_n \lor \neg Imp_n))</span>.</p>
    <p>Impostors do not announce anything at this stage. Since crewmates only incorporate the knowledge of other agents that they are sure of are also crewmates, the announcements of the impostors would always be ignored and are therefore not of any added value. </p>

    <h4>Voting Phase</h4>
    <p>The crewmates and the impostors have different voting behaviour. Before voting, the crewmates check in the kripke model which worlds they still consider to be possible and therefore also which agents they still suspect. They also check if they are already sure whether an agent is the impostor. If this latter is the case, then they will vote for this agent which they know is the impostor. Otherwise, they randomly vote for an agent which is still on their suspect-list, or they pass. When the list of suspects becomes smaller, so does the chance of crewmates passing. This allows crewmates to be more confident in their guess if there are fewer impostor possibilities. The threshold under which an agent passes is determined by the number of suspects divided by half of the initial agents.</p>
    <p>The impostors use higher-order knowledge in their voting strategy. Before voting, they determine which crewmate they should vote off by counting the number of suspects these crewmates still have. This number is indirectly announced in the announcement phase, and from this information impostors know what crewmates know. In order to eliminate the highest threats, the impostors vote for the agent that suspects the least number of agents and is therefore closest to finding the impostors. The impostors never vote for fellow impostors.</p>
    <p>After all votes have been cast, the action with most votes is executed. This means that either an agent is voted off the ship, or the voting round is passed. In case a tie occurs between multiple actions, the voting round also passes. There does not need to be a majority (e.g. more than half of the votes) for an action to be executed.</p>

    <h4>Ending conditions</h4>
    <p>The game can end in three possible ways:</p>
    <ul>
    <li><p>All impostors are voted out, meaning that the crewmates win.</p></li>
    <li><p>All crewmates completed all of their assigned tasks, meaning that the crewmates win.</p></li>
    <li><p>An equal or lesser number of crewmates than impostors are alive, meaning that the impostors win. In such a situation, the crewmates are not capable of voting off the impostors anymore, assuming that the impostors do not vote for other impostors.</p></li>
    </ul>

  <a id="formal_model"></a>
  <h2>Formal Model</h2>
    <p> Our simulation allows for one or two impostors, both with differing Kripke models. The formal definition for both will be given here. First, shared in both definitions are the following: </p>
    <ul>
    <li><p>A set of m agents <span class="math inline">A = \{a_1, a_2, ..., a_m\}</span></p></li>
    <li><p>A set of n rooms <span class="math inline">X = \{r_1, r_2, ..., r_n\}</span></p></li>
    <li><p>A set of timesteps <span class="math inline">Y = \mathbb{N}</span></p></li>
    <li><p>A set of predicates <span class="math inline">P = \{Imp_1, Imp_2, ..., Imp_m \}</span></p></li>
    </ul>
    <br/>
  <h3>Kripke Model: One impostor</h3>
  <p>The Kripke model for the case of one impostor is defined as follows:</p>
  <p><span class="math inline">M^1 ::= &lt; S^1, \pi^1, R_1^1, ..., R_m^1 &gt;</span>, with:</p>
  <ul>
    <li><p><span class="math inline">S^1 = \{s_i | a_i \in A\}</span></p></li>
    <li><p><span class="math inline">\pi^1(s_i)(Imp_j) = t</span> iff <span class="math inline">i = j</span></p></li>
    <li><p><span class="math inline">R_b^1 = \{(s_i, s_j) | s_i \in S^1 \land s_j \in S^1 \land i &ne; b \land j &ne; b\} \cup \{(s_i, s_j) | s_i \in S^1 \land i = j\}</span> for all <span class="math inline">b</span> such that <span class="math inline">a_b \in A</span>. <br/> In other words: The relations for an agent are the pairs of worlds in which the number corresponding to the agent does not appear, unioned with the set of reflexive relations.</p></li>
  </ul>
  <h3>Kripke Model: Two impostors</h3>
  <p>The Kripke model for the case of two impostors is defined as follows:</p>
  <p><span class="math inline">M^2 ::= &lt; S^2, \pi^2, R_1^2, ..., R_m^2 &gt;</span>, with:</p>
  <ul>
    <li><p><span class="math inline">S^2 = \{s_{i-j} | a_i \in A \land a_j \in A \land i &lt; j\}</span></p></li>
    <li><p><span class="math inline">\pi^2(s_{i-j})(Imp_k) = t</span> iff <span class="math inline">i = k</span> or <span class="math inline">j = k</span></p></li>
    <li><p><span class="math inline">R_b^2 = \{(s_{i-j}, s_{k-l}) | s_{i-j} \in S^2 \land s_{k-l} \in S^2 \land i &ne; b \land j &ne; b \land k &ne; b \land l &ne; b \} \cup \{(s_{i-j}, s_{k-l}) | s_{i-j} \in S^2 \land s_{i-j} = s_{k-l}  \}</span> for all <span class="math inline">b</span> such that <span class="math inline">a_b \in A</span></p></li>
  </ul>

  <a id="kripke_model"></a>
  <h3>Kripke Model: Number of worlds and relations</h3>
    <p>As aforementioned, our implementation of Among Us supports simulations with either one or two impostor(s). The size of the Kripke model The size of the model depends on this, as each world represents which single agent is, or pair of agents are, the impostor(s). For one impostor, the model has the same number of worlds as there are agents, where each world represents a state where one agent is the impostor. Each world is connected to each world, meaning there are <span class="math inline">10*10 =100</span> relations. </p>
    <p>For two impostors, the size of the model is the number of agents times the number of agents minus one, divided by two. In the case of ten agents, this means that there are <span class="math inline">(10*9)/2=45</span> possible worlds. This way, all possible combinations of agents have a single world. This means that <span class="math inline">w_1_2</span> does exist but <span class="math inline">w_2_1</span> does not. As for the relations, for each agent, each world in which that agent is not involved (45-9=36) is connected to all other worlds minus itself (35). Additionally, each world has a reflexive relation to itself. There are therefore <span class="math inline">(36*35)+45=1305</span> relations per agents, so <span class="math inline">10*1305=13050</span> relations in total. At the start of the game, the Kripke Model for ten agents and two impostors can be seen in Fig.2.</p>
    <figure>
      <img src="site_images/KMFULL.png" alt="Kripke Model at the start of a game" style="max-width: 100%; height: auto; width: 800px">
      <figcaption>Fig.2 - The full kripke model at the start of a ten agent game with two impostors.</figcaption>
    </figure>

    <p>In the visualization of the world, the reflexive arrows have been left out and the straight lines represent bidirectional arrows. Additionally, the impostor relations have also been left out of the in-game simulation. This is because the impostors do not update these relations, nor are they used by any other agents. This allows for a neater visualisation, as otherwise all relations would still be visible by at the end of the game. At the end of an example run, the Kripke Model then looks like Fig.3.</p>

    <figure>
      <img src="site_images/KMEnd.png" alt="Kripke Model after a run" style="max-width: 100%; height: auto; width: 800px">
      <figcaption>Fig.3 - The full kripke model after a random ten agent game with two impostors.</figcaption>
    </figure>

  <a id="knowledge_types"></a>
  <h2>Knowledge Types</h2>
    <p>There are four basic types of knowledge that can be learned. Here, a<sub>1</sub> and a<sub>2</sub> $\in A$, x<sub>1</sub> and x<sub>2</sub>$\in X$ and y $\in Y$ <a href="#formal_model">(see the 'formal model' section)</a>:</p>
    <ul>
      <li>Whether an agent performed a visual task. For example, a<sub>1</sub> performed a visual task in room x<sub>1</sub> at time y.</li>
      <li>Whether an impostor killed a crewmate. For example, a<sub>1</sub> was killed by a<sub>2</sub> in room x<sub>1</sub> at time y.</li>
      <li>Whether an agent is dead or alive. For example, a<sub>1</sub> is alive at time y.</li>
      <li>Whether an agent is an impostor or a crewmate. For example, a<sub>1</sub> is an impostor.</li>
      </ul>
    <p>For the first two in these types of knowledge to be gained, it is important that a crewmate a<sub>1</sub> is in the same room as agent a<sub>2</sub> (where a<sub>1</sub> <span>&#8800;</span> a<sub>2</sub>) at the same time. The last knowledge type about whether an agent is an impostor or not can be derived from the first three knowledge types as described in the next sections.</p>

  <a id="reasoning_rules"></a>
  <h2>Reasoning Rules</h2>
  <p>There are several rules a crewmate (here a<sub>1</sub>, a<sub>2</sub> and a<sub>3</sub>) can use to deduct this last type of knowledge about who the impostors (here a<sub>4</sub>) are. They are listed as follows:</p>
  <ul>
      <li>Catching the impostor in the act: ((a<sub>1</sub> $\land$ a<sub>4</sub>) in room x<sub>1</sub> at step y $\land$ a<sub>4</sub> kills a<sub>2</sub>) $\rightarrow$ K<sub>1</sub> Imp<sub>4</sub></li>
      <li>Clearing a crewmate by seeing their visual task: ((a<sub>1</sub> $\land$ a<sub>2</sub>) in room x<sub>1</sub> at step y $\land$ a<sub>2</sub> performs a visual task in room x<sub>1</sub> at step y) $\rightarrow$ K<sub>1</sub> $\neg$Imp<sub>2</sub>.</li>
      <li>Dead agents must be crewmates: a<sub>1</sub> is dead $\rightarrow$ C $\neg$Imp<sub>1</sub></li>
      <!-- <li>Trust friendly announcement: (K<sub>1</sub> $\neg$Imp<sub>2</sub> $\land$ a<sub>2</sub> announces K<sub>2</sub>($\neg$Imp<sub>3</sub> $\land$ </sub>Imp<sub>4</sub>)) $\rightarrow$ K<sub>1</sub>($\neg$Imp<sub>3</sub> $\land$ </sub>Imp<sub>4</sub>). Because of higher order knowledge, a<sub>1</sub> now also knows that a<sub>3</sub> is a crewmate and a<sub>4</sub> is an impostor.</li>
      -->
      <li><p>Trust friendly announcements. If $a_1 \in A$ knows that $a_2 \in A$ is not an impostor, and $a_2$ shares information, then $a_1$ takes that information as factual. This is equivalent to the announcement: <br/> $[k_i (\neg Imp_j) \land k_i k_j \psi] k_i \psi$ for all $i, j$ such that $a_i \in A$ and $a_j \in A$.</p></li>
  </ul>

  <a id="findings"></a>
  <h2>Findings</h2>
    <p>In order to test several functionalities and interactions of our implementation, we ran several simulations. The standard settings with which we ran the simulations are: </p>
    <ul>
      <li>Number of impostors: 2</li>
      <li>Number of crewmates: 8</li>
      <li>Number of tasks: 10</li>
      <li>... of which visual: 50%</li>
      <li>Cooldown (timesteps): 5</li>
    </ul>

    <p>Each simulation was run with 10000 iterations and the standard settings were used, except for the variable that was experimented with.</p>
    <p>The first changeable variable is the number of impostors (see Fig.4). Our implementation supports having either one or two impostors. Here, as expected, the crewmates are very likely to win in a game with only one impostor. In most cases, the crewmates figure out who the impostor is and vote them out. In some cases, the crewmates win by completing their tasks. This means that the impostor was not fully discovered, but could also not kill enough crewmates in time. In about 3% of all cases, the single impostor still manages to kill enough crewmates in order to win.</p>
    <p>The game with two impostors, which is run according to all the standard settings, is more balanced. Here, the impostor/crewmate winrate is close to 50/50. This property, in combination with standard settings from the real game Among Us, is why these base settings were set this way. Where crewmates win, they usually do so because they discovered who the impostors were and could therefore vote them out. In about 13% of all runs, the impostors could not kill fast enough, but were also not discovered in time. This results in a win for the crewmates because they managed to finish all their tasks and save the ship.</p>

    <figure>
      <img src="site_images/plots/varying_imps.png" alt="Results with a varying number of impostors" style="max-width: 100%; height: auto; width: 800px">
      <figcaption>Fig.4 - The winrate of impostors and crewmates depending on the number of impostors.</figcaption>
    </figure>

    <p>When varying the number of crewmates (see Fig.5), the winrate shifts a lot. With three crewmates and two impostors, it is near impossible for the crewmates to win. This is because if only one of them is killed, they already lose. When increasing the number of crewmates on the other hand, it becomes apparant that the impostors have a hard time winning. Another visible change is that the crewmates are more likely to win by voting out the impostors than by completing all their tasks. This is due to the fact that (1) impostors have to kill more people and therefore have a higher chance of being caught, (2) it takes longer for all the impostors to kill all crewmates, giving the crewmates more time to communicate, and (3) crewmates have more possible agents that they can trust and communicate with. Therefore, their combined knowledge is greater and the impostors can more easily be identified.</p>

    <figure>
      <img src="site_images/plots/varying_crews.png" alt="Results with a varying number of crewmates" style="max-width: 100%; height: auto; width: 800px";">
      <figcaption>Fig.5 - The winrate of impostors and crewmates depending on the number of crewmates.</figcaption>
    </figure>

    <p>Varying the number of tasks (see Fig.6) also behaves as can be expected. With a lower number of tasks (e.g. four), the crewmates mainly win by completing their tasks, as the impostors do not have enough time to kill and the crewmates do not have enough time to gain and share their knowledge. With an increasing number of tasks however, the chances of crewmates winning by finishing their tasks becomes smaller and smaller. With 16 tasks, this becomes near impossible. The increasing number of tasks also increases the chances of impostors winning, as they have more time to kill, but it also increases the chances of crewmates winning by voting off the impostors. With more tasks, the number of visual tasks also scales. This allows the crewmates to clear eachother more often, and they can therefore share their knowledge more often. Additionally, they have extra time to gather and share this information.</p>

    <figure>
      <img src="site_images/plots/varying_tasks.png" alt="Results with a varying number of tasks" style="max-width: 100%; height: auto; width: 800px">
      <figcaption>Fig.6 - The winrate of impostors and crewmates depending on the number of tasks.</figcaption>
    </figure>

    <p>The number of visuals greatly impacts the trust relations between agents, as can be seen on the left of Fig.7. With no visual tasks, the crewmates can never establish full trust in another agent. Therefore, they cannot know that what other agents communicate is true, making them vulnerable to losing from the impostors, as can be seen in the figure on the right of Fig.7. With an increasing number of visual tasks, where five visual tasks means that half of the tasks are visual and ten visual tasks means that all tasks are visual, the winratio for the crewmates quickly increase, as their chances of trusting and communicating with other crewmates also increase. This is visible by the decline in impostor wins and the decreasing part the wins by tasks play in their total number of wins.</p>

    <figure>
    <div class="row">
      <div class="column">
        <img src="site_images/plots/varying_cooldown_trusts.png" alt="Results with a varying number of visual tasks" style="max-width: 100%; height: auto; width: 800px">
      </div>
      <div class="column">
        <img src="site_images/plots/varying_visuals.png" alt="Results with a varying number of visual tasks" style="max-width: 100%; height: auto; width: 800px">
      </div>
    </div>
    <figcaption>Fig.7 - The winrate of impostors and crewmates as well as the total trust relations depending on the number of visual tasks.</figcaption>
    </figure>

    <p>The last variable that we changed in our simulations was the kill cooldown of the impostors (see Fig.8). This is denoted in the number of time steps that pass before an impostor can kill again. Interestingly, when the impostors do not have any kill cooldown, they are less likely to win than when they do have a (short) kill cooldown. This can mostly be explained by how the impostors decide they are going to kill. Basically, the more agents that are in the room, the less likely an impostor is to kill one of these agents, as these agents would otherwise know who the impostor is. However, this probability is not zero, so with such a short cooldown, the impostors also have a higher probability of killing off agents when too many other agents are around. This can for example occur right at the start of the game, when all agents gather in the Cafetaria. Additionally, more frequent kills lead to more frequent discussion phases, which allows crewmates to communicate and vote more often. In general, this leads to shorter games, which is also illustrated by the fact that next to no games are won by completing all the tasks. With a longer cooldown like ten seconds, the behaviour is as expected, where the impostors win less than with a five-step cooldown and the crewmates win more often by completing all their tasks. With such a set-up, the impostors can often not kill the crewmates in time. </p>
    <figure>
      <img src="site_images/plots/varying_cooldown.png" alt="Results with a varying kill cooldown" style="max-width: 100%; height: auto; width: 800px">
      <figcaption>Fig.8 - The winrate of impostors and crewmates depending on the kill cooldown.</figcaption>
    </figure>

<!-- TODO: Jeroen add section -->
  <a id="extended-knowledge-model-movement-between-rooms"></a>
  <h2>Extended Knowledge Model: Movement between Rooms</h2>
  <p> In this section, we will discuss a model for formalizing the movement that takes place in among us. Due to technical limitations, this was not added to the simulations, but rather explored theoretically. </p>

  <h3 id="background" class="unnumbered">Background</h3>
  <p>One kind of reasoning about the game Among Us involves the way players move from room to room. For example, consider a scenario where player <span class="math inline">c</span> learns that <span class="math inline">b</span> was killed in a specific room and tries to figure out who could have been the killer. He might for example think as follows: <em>“I was close to player <span class="math inline">a</span> while <span class="math inline">b</span> was killed at the other side of the map. This means <span class="math inline">a</span> cannot be the killer.”</em> A good player does not just consider the information that he observed directly, but also what can be extrapolated from it by considering the rules of the game. For example, <span class="math inline">a</span> might have been out of sight of <span class="math inline">c</span> for a short time, but <span class="math inline">c</span> can use his knowledge of the map to deduce that there was insufficient time for <span class="math inline">a</span> to move all the way to the place where <span class="math inline">b</span> was killed. In this case <span class="math inline">c</span> should still remove <span class="math inline">a</span> from his list of suspects.</p>
  <p>We can capture this kind of reasoning by constructing a Kripke model that describes what agents know about where everyone is at any time. To keep things simple we will study a highly simplified version of the game in which we only consider the way agents move from room to room. Movement and observations will be described by using Dynamic Epistemic Logic action models <span class="citation" data-cites="Ditmarsch2007DynamicEL">(Ditmarsch, Hoek, and Kooi 2007)</span> with postconditions<span class="citation" data-cites="postconditions">(Benevides and Lima 2017)</span>. This approach could in principle be extended to a more complex model, where we also consider things like other types of actions and agent roles, although in that case the number of states would probably become very large even when considering modest numbers of agents and rooms. In what follows we will first describe the Kripke model and the action models used to update it. We will then describe how the model was implemented in Python, describe two optimizations that are used in the implementation, and show its application to a simple example. Our source code is available on Github and can also be executed interactively using a web-based notebook on Binder without the need to install any software.</p>
  <h3 id="kripke-model" class="unnumbered">Kripke model</h3>
  <p>Let <span class="math inline">\mathcal{A}</span> denote the set of agents and <span class="math inline">\mathcal{R}</span> the set of rooms. At any time, each agent is in one of a fixed set of rooms. We describe the location of agents with the set of atomic propositions <span class="math inline">P_{\mathcal{M},\mathcal{R}} = \{a_r | a \in \mathcal{A}, r \in \mathcal{R}\}</span>, where <span class="math inline">a_r</span> denotes that <span class="math inline">a</span> is in room <span class="math inline">r</span>. The way agents can move from room to room is described by the relation <span class="math inline">\rightsquigarrow</span> on the rooms, such that <span class="math inline">r_1 \rightsquigarrow r_2</span> if room <span class="math inline">r_2</span> can be reached from room <span class="math inline">r_1</span> in one movement step. Since we will always allow agents to move in both directions between rooms and to remain in the same room, the relation <span class="math inline">\rightsquigarrow</span> will be reflexive and symmetric.</p>
  <p>When <span class="math inline">|\mathcal{A}| = m</span>, at any given time the knowledge in our model is described by a <span class="math inline">\mathcal{S}5_{\mathsf{(m)}}</span>-world <span class="math inline">(\mathbb{M}, s)</span>, where <span class="math inline">\mathbb{M} = \left&lt;S, \pi, R_1, \dots, R_m \right&gt;</span> is a Kripke structure and the distinguished state <span class="math inline">s \in S</span> describes the real world. The valuation function <span class="math inline">\pi: S \rightarrow (P_{\mathcal{M},\mathcal{R}} \rightarrow \{\mathbf{t},\mathbf{f}\})</span> assigns a truth value to all atomic propositions in each state. Because agents are always in exactly one room, the valuation function must satisfy the following constraint for every <span class="math inline">a \in \mathcal{A}</span> in order to describe a consistent state of the game:</p>
  <p><span class="math display">[\pi(s)(a_i) = \mathbf{t}] \quad \mathrm{iff} \quad [\pi(s)(a_j) = \mathbf{f}] (j \neq i)</span></p>
  <h3 id="action-models" class="unnumbered">Action models</h3>
  <p>The Kripke model is updated by executing an action model with preconditions and postconditions. An action model describes a set of possible actions, of which one will be executed. The definitions we use are similar to those given in <span class="citation" data-cites="postconditions">(Benevides and Lima 2017)</span>, but the preconditions are restricted to being conjunctions of possibly negated atomic propositions, which is sufficient for our prupose. Briefly, an action model with postconditions is very similar to a Kripke structure and has the following structure: <span class="math inline">\mathsf{M} = \left&lt;\mathsf{S}, \mathsf{pre}, \mathsf{pos}, R_1, \dots, R_m\right&gt;</span>. Here <span class="math inline">\mathsf{S}</span> is the set of actions that are possible in the action model. <span class="math inline">\mathsf{pre} : \mathsf{S} \rightarrow \mathcal{L}</span> assigns a precondition sentence to each action (something like <span class="math inline">p_1 \land p_2 \land \neg p_3</span>), and <span class="math inline">\mathsf{post} : \mathsf{S} \rightarrow (\{p_1, \dots\ p_j \}, \{\neg q_1, \dots, \neg q_k\})</span> assigns to each state a set of propositions that will become <em>true</em> after the corresponding action is executed and a set of propositions that will become <em>false</em>. The relations <span class="math inline">R_m</span> relate states that are indistinguishable for agent <span class="math inline">m</span>.</p>
  <p>The result of executing an action <span class="math inline">(\mathsf{M}, \mathsf{j})</span> in action model <span class="math inline">(\mathbb{M}, s)</span> is the Kripke world (<span class="math inline">\mathbb{M} \otimes \mathsf{M}, (s, \mathsf{j}))</span>. We will not repeat the definition of the operation, but the states <span class="math inline">(s, \mathsf{j})</span> of <span class="math inline">\mathbb{M} \otimes \mathsf{M}</span> are those states in the Caresian product <span class="math inline">S \times \mathsf{S}</span> where <span class="math inline">(mathbb{M}, s) \vDash pre(\mathsf{j})</span>, with the substitutions in the postcondition carried out. Two states in the product model are related for agend <span class="math inline">a</span> if the corresponding states and actions are related for that agent in <em>both</em> the Kripke model and the action model (thus agents can distinguish resulting states when they can distinguish either the previous state or the action).</p>
  <p>In our simplified version of Among Us, every turn consists of all agents simulataneously taking a movement step, followed by them simultaneously observing the state of the new room they are in. We can neatly define this set of operations as the execution of a set of action models. We will define a movement action model <span class="math inline">\mathsf{MOV}_a</span> describing agent <span class="math inline">a</span> taking one step, and an action model <span class="math inline">\mathsf{OBS}</span> for the effect of all observations. The relation between the Kripke models of two consecutive time steps is then as follows:</p>
  <p><span class="math display">(\mathbb{M}_{t+1}, s_{t+1}) = (\mathbb{M}_{t}, s_{t}) \otimes (\mathsf{MOV}_{a}, \mathsf{mov}_{a,t}) \otimes \dots \otimes (\mathsf{MOV}_{m}, \mathsf{mov}_{m,t}) \otimes (\mathsf{OBS}, \mathsf{obs}_{t})</span></p>
  <h3 id="single-agent-movement" class="unnumbered">Single agent movement</h3>
  <p>The single agent movement action model <span class="math inline">\mathsf{MOV_a}</span> contains one state for every connected pair of rooms <span class="math inline">(r, s) \in \rightsquigarrow</span>. It has precondition <span class="math inline">\mathsf{pre}(r,s) = a_r</span>, and postcondition <span class="math inline">\mathsf{post}(r,s) (\{a_s\}, \{\neg a_r\})</span> (so the only change is the position of agent <span class="math inline">a</span>). All states are distinguishable for <span class="math inline">a</span> and indistinguishable for all other agents, so <span class="math inline">R_x</span> includes only the reflexive relations if <span class="math inline">x=a</span> and is the full relation on the states of <span class="math inline">\mathsf{MOV_a}</span> for all other agents.</p>
  <h3 id="observation" class="unnumbered">Observation</h3>
  <p>The observation action model contains one action for every possible distribution of agents over rooms, so the total number of states is <span class="math inline">|\mathcal{R}| ^ {|\mathcal{A}|}</span>. For the state for a distribution <span class="math inline">(a_i, b_j, \dots)</span>, the precondition will be the conjunction <span class="math inline">a_i \land b_j \land \dots</span>, so each action picks out states in the Kripke model that match that distribution of agents. Two states in the observation action model are equivalent for agent <span class="math inline">a</span> if the set of agents in the room where <span class="math inline">a</span> is is the same. The postconditions of the observation action model is empty. Thus, applying this action model leaves the size of the Kripke model unchanged but removes relations where agents can distinguish the corresponding states based on what they can see in the room they are in.</p>
  <h3 id="implementation" class="unnumbered">Implementation</h3>
  <p>The Kripke models and action models were implemented from scratch in Python using the <code>NetworkX</code> graph manipulation library<span class="citation" data-cites="NetworkX">(Hagberg, Schult, and Swart 2008)</span>. As we only consider fairly simple sentences here, all logical sentences are evaluated using the built-in set operations of Python. To be able to handle more complex sentences (like higher-order knowledge) the Kripke models could be translated into <code>mlsolver</code> models like the other part of our project, but for the sake of simplicity we did not pursue this.</p>
  <h3 id="optimization-1-removing-irrelevant-states" class="unnumbered">Optimization 1: Removing irrelevant states</h3>
  <p>Because we only evaluate sentences that contain a single <span class="math inline">K_a</span>-operator in the distinguished state <span class="math inline">s</span> and this operator only depends on the states that are reachable from <span class="math inline">s</span> in one step, the ourcomes we report do not depend on the states in the Kripke-model that are not related to <span class="math inline">s</span> for any agent. We can use this fact by removing all states that are not neighbours of <span class="math inline">s</span> after every application of an action model. It is easy to see that this simplification is safe as long as the preconditions and postconditions in the action model don’t use the <span class="math inline">K</span>-operator (which is currently not possible in our simulation). This optimization can be enabled or disabled in the simulation, and was used during the generation of the results below.</p>
  <h3 id="optimization-2-replacing-nodes-with-their-bisimulation-class" class="unnumbered">Optimization 2: Replacing nodes with their bisimulation class</h3>
  <p>After the execution of a sequence of action models, one state will be created for each sequence of actions that is allowed by the preconditions. This means the number of states tends to grow as the simulation proceeds. Many of these states can be removed by the first optimization, but an orthagonal problem is the creation of states that are equivalent for the evaluation of all sentences. This type of equivalence is called a bisimulation, and we can deal with it by replacing all states in the Kripke-model with their bisimulation class. A full explanation is given in <span class="citation" data-cites="Bisimulations">(Eijck 2006)</span>, in addition to an algorithm that can be used to perform the optimization. We implemented this algorithm and saw a simplification of the Kripke model in many cases. For example, when the simulation is started with two agent in the same room that are then moved apart and back together, both agents know the full state of the game, but the Kripke model will consist of a number of indistinguishable states referring to each other. The application of the bisimulation algorithm correctly reduces these to a single state. In our example below, this step was performed after the removal of irrelevant sates every time an action model was executed. In the implementation of the bisimulation algorithm an existing recipe for partitioning sets into equivalence classes based on a binary predicate was used <span class="citation" data-cites="partition">(Reid 2007)</span> together with the graph algorithms built into <code>NetworX</code>.</p>
  <h3 id="example" class="unnumbered">Example</h3>
  <p>For the example, we consider a game where two agents <span class="math inline">a</span> and <span class="math inline">b</span> are moving around in a map of four room that are connected in a linear fashion (<span class="math inline">r_1 \leftrightsquigarrow  r_2 \leftrightsquigarrow r_3 \leftrightsquigarrow r_4</span>). In the first time step, both agents are in room 1. Every consecutive step, agent <span class="math inline">b</span> moves one step to the right while agent <span class="math inline">a</span> remains in room 1. Becuase the Kripke models are not very informative to look at directly, we have at each time step evaluated propositions of form <span class="math inline">a_i, \neg a_i, K_{a^\prime} a_i, K_{a^\prime} \neg a_i</span> in the distinguished state <span class="math inline">s</span> (the real world) for every combination of <span class="math inline">a \in \{a, b\}, a^\prime \in \{a, b\}, i \in \{1,2,3,4\}</span>. The code includes a function for automatically evaluating such a set of propositions and constructing a LaTeXtable that can be used to easily check the state of the Kripke model.</p>
  <p>The results are shown in the tables below, where all propositions in the <em>truth</em> row should be read as <span class="math inline">(\mathbb{M}_t,s_t) \vDash p</span>, in the <em>agent a</em> row as <span class="math inline">(\mathbb{M}_t,s_t) \vDash K_ap</span> and in the <em>agent b</em> row as <span class="math inline">(\mathbb{M}_t,s_t) \vDash K_bp</span> (so each table cell shows propositions describing that room that are <em>true</em> or that are <em>known to be true by that agent</em>). <span class="math display">\begin{array}{|c|c|c|c|c|}
  \hline
  t = 1 &amp; \text{Room 1}&amp;\text{Room 2}&amp;\text{Room 3}&amp;\text{Room 4}\\
  \hline
  \text{Truth} &amp; a_1,b_1 &amp; \neg b_2,\neg a_2 &amp; \neg b_3,\neg a_3 &amp; \neg a_4,\neg b_4\\
  \hline
  \text{Agent a} &amp; a_1,b_1 &amp; \neg b_2,\neg a_2 &amp; \neg b_3,\neg a_3 &amp; \neg a_4,\neg b_4\\
  \hline
  \text{Agent b} &amp; a_1,b_1 &amp; \neg b_2,\neg a_2 &amp; \neg b_3,\neg a_3 &amp; \neg a_4,\neg b_4\\
  \hline
  \end{array}</span></p>
  <p>Initially, agent <span class="math inline">a</span> and <span class="math inline">b</span> can see each other, so both know where the other is, and consequently the entire state of the game.</p>
  <p><span class="math display">\begin{array}{|c|c|c|c|c|}
  \hline
  t=2 &amp; \text{Room 1}&amp;\text{Room 2}&amp;\text{Room 3}&amp;\text{Room 4}\\
  \hline
  \text{Truth} &amp; a_1,\neg b_1 &amp; b_2,\neg a_2 &amp; \neg b_3,\neg a_3 &amp; \neg a_4,\neg b_4\\
  \hline
  \text{Agent a} &amp; a_1,\neg b_1 &amp; b_2,\neg a_2 &amp; \neg b_3,\neg a_3 &amp; \neg a_4,\neg b_4\\
  \hline
  \text{Agent b} &amp; a_1,\neg b_1 &amp; b_2,\neg a_2 &amp; \neg b_3,\neg a_3 &amp; \neg a_4,\neg b_4\\
  \hline
  \end{array}</span></p>
  <p>At time 2, <span class="math inline">b</span> has moved one step to the right (as can be read from the <em>truth</em> row). After one step of <span class="math inline">b</span>, agent <span class="math inline">a</span> knows that <span class="math inline">b</span> must be in room 2 because he is no longer in room 1 and this is the only room that was reachable in one step. Similarly, agent <span class="math inline">b</span> sees that <span class="math inline">a</span> is not in room 2. Because the only possible actions for <span class="math inline">a</span> were to move to room 2 or to stay in room 1, he knows <span class="math inline">a</span> must still be in room 1. Consequently, both agents still know the valuation of all atomic propositions.</p>
  <p><span class="math display">\begin{array}{|c|c|c|c|c|}
  \hline
  t = 3 &amp; \text{Room 1}&amp;\text{Room 2}&amp;\text{Room 3}&amp;\text{Room 4}\\
  \hline
  \text{Truth} &amp; a_1,\neg b_1 &amp; \neg b_2,\neg a_2 &amp; b_3,\neg a_3 &amp; \neg a_4,\neg b_4\\
  \hline
  \text{Agent a} &amp; a_1,\neg b_1 &amp; \neg a_2 &amp; \neg a_3 &amp; \neg a_4,\neg b_4\\
  \hline
  \text{Agent b} &amp; \neg b_1 &amp; \neg b_2 &amp; b_3,\neg a_3 &amp; \neg a_4,\neg b_4\\
  \hline
  \end{array}</span></p>
  <p>After another time step, <span class="math inline">a</span> has no way to know if <span class="math inline">b</span> stayed in room 2 or moved to room 3. Consequently, all he knows about <span class="math inline">b</span> is that he is not in room 1 (or he would be seen by <span class="math inline">a</span>) or in room 4 (because that room cannot be reached from room 1 in two steps). Agent <span class="math inline">b</span> also doesn’t know exactly where <span class="math inline">a</span> is anymore, but he does know <span class="math inline">a</span> cannot be in room 3 or 4.</p>
  <p><span class="math display">\begin{array}{|c|c|c|c|c|}
  \hline
  t = 4 &amp; \text{Room 1}&amp;\text{Room 2}&amp;\text{Room 3}&amp;\text{Room 4}\\
  \hline
  \text{Truth} &amp; a_1,\neg b_1 &amp; \neg b_2,\neg a_2 &amp; \neg b_3,\neg a_3 &amp; b_4,\neg a_4\\
  \hline
  \text{Agent a} &amp; a_1,\neg b_1 &amp; \neg a_2 &amp; \neg a_3 &amp; \neg a_4\\
  \hline
  \text{Agent b} &amp; \neg b_1 &amp; \neg b_2 &amp; \neg b_3 &amp; b_4,\neg a_4\\
  \hline
  \end{array}</span></p>
  <p>Finally, agent <span class="math inline">b</span> moves to the last room. Both agents now only kow that the other is not in the same room as they are.</p>
  <h3 id="references" class="unnumbered">References</h3>
  <div id="refs" class="references">
  <div id="ref-postconditions">
  <p>Benevides, Mario, and Isaque Lima. 2017. “Action Models with Postconditions.” <em>Computacion Y Sistemas</em> 21 (September): 401–6. <a href="https://doi.org/10.13053/CyS-21-3-2808">https://doi.org/10.13053/CyS-21-3-2808</a>.</p>
  </div>
  <div id="ref-Ditmarsch2007DynamicEL">
  <p>Ditmarsch, H. V., W. Hoek, and Barteld P. Kooi. 2007. “Dynamic Epistemic Logic.” In.</p>
  </div>
  <div id="ref-Bisimulations">
  <p>Eijck, Jan van. 2006. “Lecture Notes Logica Voor Ai: Bisimulations.” <a href="https://staff.fnwi.uva.nl/d.j.n.vaneijck2/courses/lai0506/LAI11.pdf">https://staff.fnwi.uva.nl/d.j.n.vaneijck2/courses/lai0506/LAI11.pdf</a>.</p>
  </div>
  <div id="ref-NetworkX">
  <p>Hagberg, Aric A., Daniel A. Schult, and Pieter J. Swart. 2008. “Exploring Network Structure, Dynamics, and Function Using Networkx.” In <em>Proceedings of the 7th Python in Science Conference</em>, edited by Gaël Varoquaux, Travis Vaught, and Jarrod Millman, 11–15. Pasadena, CA USA.</p>
  </div>
  <div id="ref-partition">
  <p>Reid, John. 2007. “Equivalence Partition (Python Recipe).” <a href="https://code.activestate.com/recipes/499354-equivalence-partition/">https://code.activestate.com/recipes/499354-equivalence-partition/</a>.</p>
  </div>
  </div>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["jsMath2jax.js"]
    });
  </script>

  <script
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML">
  </script>

  <script>
      window.onscroll = function() {myFunction()};

      var header = document.getElementById("myHeader");
      var sticky = header.offsetTop;

      function myFunction() {
        if (window.pageYOffset > sticky) {
          header.classList.add("sticky");
        } else {
          header.classList.remove("sticky");
        }
      }

  // The function actually applying the offset
  function offsetAnchor() {
      if(location.hash.length !== 0) {
          window.scrollTo(window.scrollX, window.scrollY);
      }
  }

  // This will capture hash changes while on the page
  window.addEventListener("hashchange", offsetAnchor);

  // This is here so that when you enter the page with a hash,
  // it can provide the offset in that case too. Having a timeout
  // seems necessary to allow the browser to jump to the anchor first.
  window.setTimeout(offsetAnchor, 1); // The delay of 1 is arbitrary and may not always work right (although it did in my testing).


  </script>

</body>
</html>

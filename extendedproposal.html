<p>This extended proposal is based on a game of Among Us with only five agents, of whom one is an imposter.</p>
<h1 id="implementation-progress" class="unnumbered">Implementation progress</h1>
<p>We have implemented most of the game mechanics and a simple gui in PyGame, and our code can be found in this repository: <a href="https://github.com/MaxVinValk/AmongUsLAMAS">https://github.com/MaxVinValk/AmongUsLAMAS</a>. The agents follow a hard-coded strategy in our implementation, apart from the voting phase. Our next step is to extend the implementation with Kripke models. Here we might implement the Kripke semantics ourselves in Python, or use an existing implementation like <a href="https://github.com/erohkohl/mlsolver">https://github.com/erohkohl/mlsolver</a> if it is easy to use for our needs.</p>
<h1 id="kripke-model" class="unnumbered">Kripke Model</h1>
<p>Our proposed Kripke model for the overview of the game is relatively simple, as all agents only need to figure out which agent is the imposter. The imposter knows that they are the imposter, and all crewmates know that they are not the imposter. It would look as shown in Figure <a href="#fig:fullkripke" data-reference-type="ref" data-reference="fig:fullkripke">1</a>. Here, all straight lines represent bidirectional arrows.</p>
<figure>
<img src="images/Full Kripke Model.png" alt="The Kripke Model at the start of a Among Us game with five agents, of whom one an imposter (A)." id="fig:fullkripke" /><figcaption>The Kripke Model at the start of a Among Us game with five agents, of whom one an imposter (A).<span label="fig:fullkripke"></span></figcaption>
</figure>
<h1 id="basic-knowledge-model" class="unnumbered">Basic Knowledge Model</h1>
<p><strong>Knowledge Types</strong>: Initially, each agent only knows whether they are an imposter or not.</p>
<p>There are however types of knowledge that can be learned throughout the game. Here, <span class="math inline"><em>a</em><sub>1</sub></span> and <span class="math inline"><em>a</em><sub>2</sub></span> <span class="math inline">∈</span> set of agents A, x <span class="math inline">∈</span> set of rooms X and y <span class="math inline">∈</span> set of timesteps Y. These are types of knowledge that can be gained by any crewmate while in the action-phase:</p>
<ul>
<li><p>The location of an agent (both itself and other agents). For example, a is in room x at time y.</p></li>
<li><p>Whether an agent performed a visual task. For example, a performed a visual task in room x at time y.</p></li>
</ul>
<p>For both of these types of knowledge to be gained, it is important that a crewmate A1 is in the same room as agent A2 (where A1 <span class="math inline">$\not=$</span> A2) at the same time.</p>
<p>In the discussion phase, all agents encounter two other possible knowledge types:</p>
<ul>
<li><p>Whether an agent is dead or alive. For example, A is alive at time Y.</p></li>
<li><p>Announcements of location. For example, A announces that they were in room X at time Y.</p></li>
</ul>
<p>With these different types of knowledge, a crewmate can then reason about the final type of knowledge. Namely, whether another agent is an imposter or not.<br />
<strong>Knowledge Rules</strong>: There are several rules a crewmate (here A1) can use to deduct this final important type of knowledge. They are listed as follows:</p>
<ul>
<li><p>Catching the imposter in the act: (A1 is in the same room X as A2 at step Y <span class="math inline">∧</span> A3 is dead) <span class="math inline">→</span> A1 knows A2 is the imposter</p></li>
<li><p>Catching the imposter in a lie: (A1 is in the same room X1 as A2 at time Y <span class="math inline">∧</span> A2 announces they were at room X2 (IS NOT X1) at Y) <span class="math inline">→</span> A1 knows A2 is the imposter</p></li>
<li><p>Clearing a crewmate by seeing their task: (A1 is in the same room X as A2 at step Y <span class="math inline">∧</span> A2 performed a visual task in room X at step Y) <span class="math inline">→</span> A1 knows A2 is a crewmate</p></li>
<li><p>Dead agents must be crewmates: A1 is dead <span class="math inline">→</span> all A know that A1 a crewmate</p></li>
</ul>
<p>To model the real game, the imposter will have a cooldown of a set number of rounds on their kill-action. When it is capable of killing a crewmate, this action is stochastic. This means that simply being in the same room as another agent without getting killed does not mean that they are a crewmate.</p>
<p>In our initial knowledge model, the imposter does not yet reason about what the other agents know. We do plan to explore this using higher-order knowledge once the initial project is up and running.</p>
<h1 id="formalizing-game-states" class="unnumbered">Formalizing Game States</h1>
<p><strong>Action Phase</strong>: The action phase consists out of two phases itself: acting and observing. In the first phase, all agents can either move to an adjacent room or perform an action, which is a task if they are crewmate, or kill if they are imposter. They can also choose to perform no action and not move. This phase is repeated until a crewmate runs into a dead body, in which case the agent can report the body at the end of the action phase. This triggers the discussion phase.<br />
<strong>Discussion Phase</strong>: The discussion phase is started with an announcement of which agent has died and in which room. Then, each agent announces their location at the last five (exact number to be defined) timesteps. In case the imposter has to announce they were last in the room where the dead body was now found, the imposter will instead announce a random location. <em>Note for Jelle: Or would it be better for the imposter to repeat the location of the timestep after the kill?</em></p>
<p>Every agent can then reason using the ’catch in a lie’ rule, and finally present the agents that they still consider to be possible imposters. The imposter here announces all remaining agents minus one random agent each discussion phase. <em>Note: This might not we optimal, if we have time we can try to improve this behaviour.</em><br />
<strong>Voting Phase</strong>: In the voting phase, initially, we plan to model that each agent only votes if they know for sure who the imposter is. Otherwise, they skip the vote. If the majority then votes out the imposter, the crewmates win. We can experiment with the voting behaviour of the impostor and with different voting strategies for the crewmates. For example, agents could vote randomly on agents that they still consider suspicious instead of skipping.</p>
<h1 id="ending-conditions" class="unnumbered">Ending Conditions</h1>
<p>The game can end in three possible ways:</p>
<ul>
<li><p>The imposter is voted out by the majority, meaning that the crewmates win.</p></li>
<li><p>All crewmates completed all of their assigned tasks, meaning that they win.</p></li>
<li><p>Only one crewmate is left, which means the imposter wins.</p></li>
</ul>
<h1 id="extended-knowledge-model-movement-between-rooms" class="unnumbered">Extended knowledge model: movement between rooms</h1>
<p>In the basic knowledge model, when a crewmate learns someone was in a room he will take this into consideration, but will not use the movement rules to infer where that person could be in the previous or next step. This information is often relevant, for example when <span class="math inline"><em>a</em><sub>1</sub></span> sees <span class="math inline"><em>a</em><sub>2</sub></span> at the left of the map, and in the next time step <span class="math inline"><em>a</em><sub>3</sub></span> is killed at the right end of the map, this means <span class="math inline"><em>a</em><sub>1</sub></span> can infer that <span class="math inline"><em>a</em><sub>2</sub></span> cannot be the killer because he cannot have traveled across the map that quickly. We have thought about ways to extend our approach to make this kind of inference possible, which we might implement and compare if we have the time.</p>
<p>It is not feasible to construct a single Kripke model encoding the location of all agents at all time steps due to the curse of dimensionality. An alternative is to construct a set of disconnected models representing knowledge about subsets of the propositions about the game world. A reasonable simplification might be to construct one Kripke model for every combination of time step and agent (so for example in a game with 5 agents where 10 time steps have passed, there would be 50 such models). The location model for agent <span class="math inline"><em>a</em><sub><em>i</em></sub></span> at time <span class="math inline"><em>t</em></span> will be denoted <span class="math inline"><em>M</em><sub><em>a</em><sub><em>i</em></sub>, <em>t</em></sub><sup>loc</sup></span> and has the following propositional atoms:</p>
<p><br /><span class="math display"><em>P</em><sub><em>a</em><sub><em>i</em></sub>, <em>t</em></sub><sup>loc</sup> = {room<sub><em>a</em><sub><em>i</em></sub>; <em>r</em>; <em>t</em></sub>|<em>r</em> ∈ <em>R</em>} ∪ {imp<sub><em>a</em></sub>|<em>a</em> ∈ <em>A</em>},</span><br /> where <span class="math inline">room<sub><em>a</em><sub><em>i</em></sub>; <em>r</em>; <em>t</em></sub></span> indicates <em><span class="math inline"><em>a</em><sub><em>i</em></sub></span> was in room <span class="math inline"><em>r</em></span> at time <span class="math inline"><em>t</em></span></em> and <span class="math inline">imp<sub><em>a</em></sub></span> indicates <em>agent <span class="math inline"><em>a</em></span> is the impostor</em>.</p>
<p>We know that there is exactly one impostor in the game and that an agent is in one room at any time. Therefore, the states in a location model consist of one <span class="math inline">room</span> proposition and one <span class="math inline">imp</span> proposition. Initially, accessibility relations are those for a distributed system (agents know their own location and impostor status, but not those of others). So initially, it holds that <span class="math inline"><em>i</em> ≠ <em>j</em> ⇔ <em>M</em><sub><em>a</em>, <em>t</em></sub><sup>loc</sup> ⊨ ¬<em>K</em><sub><em>i</em></sub>imp<sub><em>j</em></sub> ∧ ¬<em>K</em><sub><em>i</em></sub>¬imp<sub><em>j</em></sub></span> unless <span class="math inline"><em>a</em><sub><em>i</em></sub></span> is the impostor (which means he knows the impostor status of all agents).</p>
<p>Now the types of knowledge obtained by the agents during the game can be translated into announcements for the location models. This will restrict the accessibility relations, and at some point some agents may acquire knowledge that someone is or is not the impostor and use this to inform their voting. A sketch of how these announcements could work is the following (announcements are made only to the agent that learn that information, which we could model with an epistemic action model):</p>
<ul>
<li><p><em>Learning agent <span class="math inline"><em>a</em></span> is in room <span class="math inline"><em>r</em></span> at time <span class="math inline"><em>t</em></span>:</em> In the location model <span class="math inline"><em>M</em><sub><em>a</em>, <em>t</em></sub><sup>loc</sup></span> this is translated as the announcement of proposition <span class="math inline">room<sub><em>a</em>; <em>r</em>; <em>t</em></sub></span>. We can also translate this announcement to other time steps, by using the connectivity between the rooms. For example we know that in the previous time step, agent <span class="math inline"><em>a</em></span> was in a room from which <span class="math inline"><em>r</em></span> was reachable, so in model <span class="math inline"><em>M</em><sub><em>a</em>, <em>t</em> − 1</sub><sup>loc</sup></span> we can announce for all rooms <span class="math inline"><em>r</em><sup>′</sup></span> that are not connected to <span class="math inline"><em>r</em></span> that <span class="math inline">¬room<sub><em>a</em>; <em>r</em><sup>′</sup>; <em>t</em> − 1</sub></span> (meaning agent <span class="math inline"><em>a</em></span> was certainly not in any of those rooms). In this way the observation can be translated to the location models of agent <span class="math inline"><em>a</em></span> for all time steps.</p></li>
<li><p><em>In the discussion phase, an agent <span class="math inline"><em>a</em></span> announces the location of an agent at some time:</em> We can exclude all states that contradict the announced location and where <span class="math inline"><em>a</em></span> is not the impostor (because only the impostor can lie). We can again use the connectivity to translate this announcement to other time steps.</p></li>
<li><p><em>Visual task is observed of some agent <span class="math inline"><em>a</em></span>:</em> We can announce to the observing agent that <span class="math inline">¬imp<sub><em>a</em></sub></span>.</p></li>
</ul>
<p>This needs to be worked out more carefully, but it would be interesting to see how much of an advantage the agents would get by adding this type of reasoning. The combined size of the models is much larger than the basic knowledge model, but seems like it should be manageable if implemented efficiently. If we choose to implement it this model can also easily be extended for the <em>venting</em> mechanic where the impostor can take shortcuts, to allow crewmates to notice the impostor moved too quickly.</p>
<p>It would be interesting to know if there is a more elegant way to deal with this kind of knowledge that creating a set of separate Kripke models.</p>
